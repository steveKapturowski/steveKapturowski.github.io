<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Strange Matter - A fledgeling blog about math, machine learning, and black holes</title>
    <description></description>
    <link>http://blog.stevenkapturowski.com/</link>
    <atom:link href="http://blog.stevenkapturowski.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>MCMC, Renormalization, and the Ising Model</title>
        <description>&lt;head&gt;
&lt;meta charset=&quot;utf-8&quot; /&gt;
&lt;style&gt;
.hide {
  display: none;
}
.rect {
  fill: none;
  stroke: hsl(0, 0%, 70%);
}
.vrange {
  margin-top: 0px;
  transform: rotate(90deg);
  -moz-transform: rotate(90deg);
}
input[type=range] {
  -webkit-appearance: none;
  margin: 14px 0;
  width: 20%;
}
input[type=range]:focus {
  outline: none;
}
input[type=range]::-webkit-slider-runnable-track {
  width: 20%;
  height: 7.4px;
  cursor: pointer;
  animate: 0.2s;
  box-shadow: 1px 1px 1px #000000, 0px 0px 1px #0d0d0d;
  background: #3071a9;
  border-radius: 1.3px;
  border: 0.2px solid #010101;
  margin-top: -12px;
}
input[type=range]::-webkit-slider-thumb {
  box-shadow: 1px 1px 1px #000000, 0px 0px 1px #0d0d0d;
  border: 1px solid #000000;
  height: 14px;
  width: 16px;
  border-radius: 3px;
  background: #ffffff;
  cursor: pointer;
  -webkit-appearance: none;
  margin-top: -3px;
}

&lt;/style&gt;
&lt;/head&gt;
&lt;center&gt;
  &lt;p&gt;
    &lt;input id=&quot;temperature&quot; type=&quot;range&quot; value=&quot;2.5&quot; min=&quot;0.0&quot; max=&quot;5.0&quot; step=&quot;0.01&quot; ticks=&quot;true&quot; /&gt;
    &lt;label style=&quot;font-size:17px&quot;&gt;Temperature [kT/J]&lt;/label&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;foo&gt;&lt;/foo&gt;
  &lt;/p&gt;
  &lt;p&gt;
    &lt;select id=&quot;model&quot;&gt;
      &lt;option value=&quot;ising&quot;&gt;Ising&lt;/option&gt;
      &lt;option value=&quot;xy&quot;&gt;XY&lt;/option&gt;
    &lt;/select&gt;
    &lt;select id=&quot;algorithm&quot;&gt;
      &lt;option value=&quot;wolff&quot;&gt;Wolff&lt;/option&gt;
      &lt;option value=&quot;metropolis&quot;&gt;Metropolis&lt;/option&gt;
    &lt;/select&gt;
  &lt;/p&gt;
&lt;/center&gt;

&lt;script src=&quot;http://d3js.org/d3.v3.min.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
(function() {
  &quot;use strict&quot;;

  var
    margin = {top: 40, right: 100, bottom: 40, left: 150},
    W = 300,
    H = 300,
    w = W - margin.left - margin.right,
    h = H - margin.top - margin.bottom,

    xmax   = 1.5,
    rate   = 1 / 1000,
    T = 2.5,
    N = 48,
    dim = W/N,

    model,
    algorithm,
    rectangles,
    lattice,
    timer;


  var svg = d3.select(&quot;foo&quot;)
    .append(&quot;svg&quot;)
      .attr(&quot;width&quot;, W)
      .attr(&quot;height&quot;, H)

  function initLattice(n) {
    var 
      newN = n,
      newDim = W/n,
      newLattice = new Array(Math.pow(newN, 2));

    for (var i=0; i &lt; Math.pow(newN, 2); i++) {
      var x = i % newN;
      var y = Math.floor(i / newN);
      var spin;

      if ( lattice == null ) {
        spin = model.randomSpin();
      } else {
        var oldX = Math.floor((x + .5) * N/newN);
        var oldY = Math.floor((y + .5) * N/newN);
        var index = oldX + oldY * N;

        var sites = getNeighbors(index);
        sites.push(index);
        spin = model.averageSites(sites);
      }
        
      newLattice[i] = {
        &quot;x&quot;: x*newDim,
        &quot;y&quot;: y*newDim,
        &quot;height&quot;: newDim,
        &quot;width&quot;: newDim,
        &quot;spin&quot;: spin,
      };
    }

    d3.select(&quot;svg&quot;).selectAll(&quot;rect&quot;).remove();
    rectangles = svg.selectAll(&quot;rect&quot;)
      .data(newLattice)
      .enter()
      .append(&quot;rect&quot;);
    model.render();

    N = newN;
    dim = newDim;
    lattice = newLattice;
  }

  function Ising () {}
  Ising.prototype.nearestNeighborInteraction = function(s1, s2) {
    return s1 * s2
  };
  Ising.prototype.flipChain = function(indices) {
    indices.forEach(function (index) { 
      lattice[index][&quot;spin&quot;] *= -1;
    });
    return indices
  };
  Ising.prototype.averageSites = function(sites) {
    var sum = sites.map(function(j) {
      return lattice[j][&quot;spin&quot;];
    }).reduce(function(a, b) {
      return a + b;
    }, 0);
    return (Math.random() &gt; (sum+5)/10) ? -1 : 1;
  };
  Ising.prototype.randomSpin = function() {
    return (Math.random() &gt; .5) ? 1 : -1;
  };
  Ising.prototype.render = function(value) {
    // rectangles = svg.selectAll(&quot;rect&quot;)
    //   .data(indices.map(function(index) {
    //     return lattice[index];
    //   })); // get rectangles whose spins have been modified
    rectangles
      .attr(&quot;x&quot;, function (d) { return d.x; })
      .attr(&quot;y&quot;, function (d) { return d.y; })
      .attr(&quot;height&quot;, function (d) { return d.height; })
      .attr(&quot;width&quot;, function (d) { return d.width; })
      .style(&quot;fill&quot;, function(d) { return (d.spin == -1) ? &quot;black&quot; : &quot;white&quot;; });
  };

  function XY () {}
  XY.prototype.nearestNeighborInteraction = function(theta1, theta2) {
    return Math.cos(theta1 - theta2)
  };
  XY.prototype.flipChain = function(indices) {
    var Ø = Math.random() * 2 * Math.PI;

    indices.forEach(function (index) { 
      lattice[index][&quot;spin&quot;] = (Math.PI + 2*Ø - lattice[index][&quot;spin&quot;]) % (2 * Math.PI);
    });
    return indices
  };
  XY.prototype.averageSites = function(sites) {
    var sum = sites.map(function(j) {
      return lattice[j][&quot;spin&quot;];
    }).reduce(function(a, b) {
      return a + b;
    }, 0);
    return sum / sites.length;
  };
  XY.prototype.randomSpin = function() {
    return Math.random() * 2 * Math.PI;
  };
  XY.prototype.render = function(value) {
    rectangles
      .attr(&quot;x&quot;, function (d) { return d.x; })
      .attr(&quot;y&quot;, function (d) { return d.y; })
      .attr(&quot;height&quot;, function (d) { return d.height; })
      .attr(&quot;width&quot;, function (d) { return d.width; })
      .style(&quot;fill&quot;, function(d) { 
        var b = Math.floor(127 + 127*Math.sin(d.spin));
        var g = Math.floor(127 + 127*Math.cos(d.spin));
        return &quot;rgb(0,&quot;+b+&quot;,&quot;+g+&quot;)&quot;; 
      });
  };

  var models = {
    ising: new Ising(), 
    xy: new XY()
  };

  function getNeighbors(index) {
    var 
      numSites = Math.pow(N, 2),
      left = index - 1,
      right = left + 2;

    if (index % N == 0) {
      left += N;
    } else if (right % N == 0) {
      right -= N;
    }

    return [
      (index + N + numSites) % numSites, //javascript&#39;s mod operation makes mathematicians cry
      (index - N + numSites) % numSites,
      left,
      right,
    ]
  }

  function growChain() {
    var
      chain = {},
      newSites = {},
      addedSites = {},
      index = Math.floor(Math.random()*Math.pow(N, 2));

    chain[index] = true;
    addedSites[index] = true;

    while ( Object.keys(addedSites).length &gt; 0 ) {
      newSites = {};
      Object.keys(addedSites).forEach(function (key) { 
        index = parseInt(key);

        getNeighbors(index).forEach(function (neighbor) { 
          if ( 
            !(neighbor in chain) 
            &amp;&amp; ( Math.random() &lt; (1 - Math.exp(-2*model.nearestNeighborInteraction(lattice[index][&quot;spin&quot;],lattice[neighbor][&quot;spin&quot;])/T)) )
          ) { 
            chain[neighbor] = true;
            newSites[neighbor] = true;
          }
        });
      });
      addedSites = newSites;
    }
    return Object.keys(chain);
  }

  var algorithms = {
    metropolis: function() {
      var 
        index = Math.floor(Math.random()*Math.pow(N, 2)),
        Edelta = 0;

      getNeighbors(index).forEach(function (neighbor) { 
        Edelta -= 2 * model.nearestNeighborInteraction(lattice[index][&quot;spin&quot;], lattice[neighbor][&quot;spin&quot;]);
      });
      if (Math.random() &lt; Math.exp(Edelta/T)) {
        model.flipChain([index]);
      }
      return [index];
    },
    wolff: function() {
      return model.flipChain(growChain()); // return modified indices
    },
  };
  algorithm = algorithms.wolff,
  model = models.ising;

  function drawGraph() {   
    model.render(algorithm());
  }

  function play() {
    if (timer) return;
    (function loop() {
      drawGraph();
      timer = setTimeout(loop, rate * 1000);
    })();
  }

  function togglePause() {
    if (!timer) {

    } else {
      clearTimeout(timer);
      timer = null;
    }
  }

  d3.select(&quot;#temperature&quot;).on(&quot;change&quot;, function() { T = this.value; });
  d3.select(&quot;#model&quot;).on(&quot;change&quot;, function() { model = models[this.value]; });
  d3.select(&quot;#algorithm&quot;).on(&quot;change&quot;, function() { algorithm = algorithms[this.value]; });

  var zoomFactor = 64;
  var zoom = d3.behavior.zoom().scaleExtent([1/16, 1]).on(&quot;zoom&quot;, function() {initLattice(2*Math.floor(zoomFactor*zoom.scale()/2)); });
  zoom.scale(N/zoomFactor);
  svg.call(zoom);

  initLattice(N);
  play();

})();
&lt;/script&gt;

&lt;p&gt;The Ising model is a simplified description of ferromagnetism where spin-up and spin-down states sit on a d-dimensional lattice with an interaction defined between spins on neighboring sites. In one and two dimensions this model is analytically soluble, but because of that it makes a perfect playground to learn about Markov Chain Monte Carlo methods; a powerful tool with diverse applications in Bayesian statistics, statistical mechanics, and quantum field theory.&lt;/p&gt;

&lt;h2 id=&quot;solving-the-ising-model-in-1d&quot;&gt;Solving the Ising Model in 1D&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; H = -\sum K_{ij}\sigma_i\sigma_j - \mu\sum J_i\sigma_i &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z = \sum_{\{\sigma\}} e^{-\beta H}&lt;/script&gt;

&lt;p&gt;As it turns out we have very nice formulas for energy, specific heat, magnetization, and susceptibility respectively: &lt;script type=&quot;math/tex&quot;&gt;E = -\frac{\partial\ln Z}{\partial \beta} &lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;c_v = k\beta^2 \frac{\partial^2\ln Z}{\partial \beta^2}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;M = -\frac{\partial\ln Z}{\partial J} \biggr\rvert_{J=0}&lt;/script&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\chi = -\frac{\partial^2\ln Z}{\partial J^2} \biggr\rvert_{J=0}&lt;/script&gt;as well as the correlation functions, all in terms of the partition function &lt;script type=&quot;math/tex&quot;&gt;Z&lt;/script&gt;. So, in principle, if we know the partition function, we know &lt;i&gt;everything&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;The problem is that the number of terms in the sum is exponential in the number of states &lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;. There are two ways to deal with the situation: 1) figure out some fancy tricks to ananlytically coerce this unwieldy beast into submission, or 2) use numerical techniques which exploit the fact that only a relatively small number of states contribute appreciably to the partition function. Here, we’ll do both.&lt;/p&gt;

&lt;p&gt;Let’s start with (1):&lt;/p&gt;

&lt;p&gt;Let N be the length of the lattice in each direction and assume periodic boundary conditions such that &lt;script type=&quot;math/tex&quot;&gt;\sigma_{N}&lt;/script&gt; neighbors &lt;script type=&quot;math/tex&quot;&gt;\sigma_{1}&lt;/script&gt;. For simplicity let’s also turn off the external magnetic field (&lt;script type=&quot;math/tex&quot;&gt;h=0&lt;/script&gt;). &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z = \sum_{\{\sigma\}} e^{K\sigma_N \sigma_1} \prod_{k=1}^{N-1} e^{K\sigma_k \sigma_{k+1}}&lt;/script&gt;

&lt;p&gt;Where &lt;script type=&quot;math/tex&quot;&gt;K = \beta J&lt;/script&gt;. Defining the &lt;i&gt;transfer matrix&lt;/i&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
 T_{\sigma_k \sigma_{k+1}} =
\begin{bmatrix}
    e^K    &amp; e^{-K} \\
    e^{-K} &amp; e^K \\
\end{bmatrix}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;we can rewrite the partition function as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z = \mbox{Tr}T^N = \lambda_1^N + \lambda_2^N &lt;/script&gt;

&lt;p&gt;Diagonalizing &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; we find eigenvalues &lt;script type=&quot;math/tex&quot;&gt;\lambda_1 = 2\cosh K&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda_2 = 2\sinh K&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;So we end up with the particularly simple form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z = (2\cosh K)^N + (2\sinh K)^N &lt;/script&gt;

&lt;h2 id=&quot;thermodynamic-limit&quot;&gt;Thermodynamic Limit&lt;/h2&gt;

&lt;p&gt;The abrubt changes in thermodynamic properties observed in phase transitions appears to be at odds with a description in terms of analytic functions. After all, the partition function is nothing more than a finite sum of exponentials. &lt;/p&gt;

&lt;p&gt;Being that &lt;script type=&quot;math/tex&quot;&gt;e^x&lt;/script&gt; is everywhere analytic and realizing that a finite sum of analytic functions is itself analytic the partition function might at first seem hopelessly incapable of describing the physics of phase transitions. But &lt;i&gt;critically&lt;/i&gt; (what a pun!), an &lt;i&gt;infinite&lt;/i&gt; sum of analytic functions need not be analytic. A familiar example comes from Fourier Series where we find that an infinite number of sine waves can add up to a square wave:&lt;/p&gt;

&lt;center&gt;
&lt;div id=&quot;canvas-container&quot; style=&quot;width: 800px; height: 300px;&quot;&gt;
    &lt;canvas id=&quot;sineCanvas&quot; style=&quot;width: 800px; height: 300px;&quot; width=&quot;800&quot; height=&quot;300&quot;&gt;&lt;/canvas&gt;
&lt;/div&gt;
&lt;/center&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://mbostock.github.com/d3/d3.js?2.6.0&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;jwerty.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
(function () {

if (typeof(Humble) == &#39;undefined&#39;) window.Humble = {};
Humble.Trig = {};
Humble.Trig.init = init;

var unit = 100,
    canvas, context, canvas2, context2,
    height, width, xAxis, yAxis,
    draw;

/**
 * Init function.
 * 
 * Initialize variables and begin the animation.
 */
function init() {
    
    canvas = document.getElementById(&quot;sineCanvas&quot;);
    
    canvas.width = 1000;
    canvas.height = 300;
    
    context = canvas.getContext(&quot;2d&quot;);
    context.font = &#39;18px sans-serif&#39;;
    context.strokeStyle = &#39;#000&#39;;
    context.lineJoin = &#39;round&#39;;
    
    height = canvas.height;
    width = canvas.width;
    
    xAxis = Math.floor(height/2);
    yAxis = Math.floor(0);
    
    context.save();
    draw();
}

/**
 * Draw animation function.
 * 
 * This function draws one frame of the animation, waits 20ms, and then calls
 * itself again.
 */
draw = function () {
    
    // Clear the canvas
    context.clearRect(0, 0, width, height);

    // Draw the axes in their own path
    context.beginPath();
    drawAxes();
    context.stroke();
    
    // Set styles for animated graphics
    context.save();
    context.strokeStyle = &#39;#000&#39;;
    context.fillStyle = &#39;#fff&#39;;
    context.lineWidth = 2;

    // Draw the sine curve at time draw.t, as well as the circle.
    context.beginPath();
    drawSine(draw.t);
    
    // Draw the arrow at time t in its own path.
    // drawArrow(draw.t);
    
    // Restore original styles
    context.restore();
    
    // Draw the xAxis PI tick and the time

    // Update the time and draw again
    draw.seconds = draw.seconds - .01;
    draw.t = draw.seconds*Math.PI;
    setTimeout(draw, 35);
};
draw.seconds = 0;

/**
 * Function to draw axes
 */
function drawAxes() {
    
    // Draw X and Y axes
    context.moveTo(0, xAxis);
    context.lineTo(width, xAxis);
    context.moveTo(yAxis, 0);
    context.lineTo(yAxis, height);
    
}

/**
 * Function to draw sine
 * 
 * The sine curve is drawn in 10px segments starting at the origin. 
 */
function drawSine(t) {

    // Set the initial x and y, starting at 0,0 and translating to the origin on
    // the canvas.

    var x = t;
    var y = Math.sin(x);
    context.moveTo(yAxis, unit*y+xAxis);
    
    // Loop to draw segments
    var last = 0;
    for (i = yAxis; i &lt;= width; i += 1) {
        x = t+(-yAxis+i)/unit;

        var sum = 0;
        var cap = Math.floor(Math.exp((i-yAxis)/(1.5*314.159)));
        for (n = 1; n &lt;= Math.pow(2, cap-1); n+= 1) {
          sum += Math.sin(2*Math.PI*(2*n-1)*x/Math.PI)/(2*n-1);
        }

        y = 4*sum/Math.PI;
        context.lineTo(i, unit*y+xAxis);

        if (cap &gt; last &amp;&amp; cap &gt;= 1) {
          last = cap
          context.stroke()
          context.beginPath();
          context.strokeStyle = &#39;#ddd&#39;;
          drawCircle(i, xAxis, unit*4/(Math.PI*(2*cap-1)));
          context.stroke()
          context.moveTo(i, unit*y+xAxis);
          
          context.beginPath();
          context.strokeStyle = &#39;#000&#39;;
        } 
    }
    context.stroke();

    context.beginPath();
    context.strokeStyle = &#39;#ddd&#39;;
    context.setLineDash([3,5])

    var last = 0
    for (i = yAxis; i &lt;= width; i += 1) {
    	var bar = Math.floor(Math.exp((i-yAxis)/(1.5*314.159)));
    	if (bar &gt; last &amp;&amp; bar &gt; 1) {
    		last = bar
    		context.moveTo(i, 0);
            context.lineTo(i, 300);
    	}
    }
    context.stroke();
}

/*
 * Function to draw circle
 */
function drawCircle(x, y, r) {
    context.moveTo(x+r, y);
    // context.arc(x, y, r, 0, 2*Math.PI, false);
}

/**
 * Function to draw arrow
 */
function drawArrow(t) {
    
    // Cache position of arrow on the circle
    var x = yAxis+unit*Math.cos(t);
    var y = xAxis+unit*Math.sin(t);
    
    // Draw the arrow line
    context.beginPath();
    context.moveTo(yAxis, xAxis);
    context.lineTo(x, y);
    context.stroke();
    
    // Draw the arrow bead
    context.beginPath();
    context.arc(x, y, 5, 0, 2*Math.PI, false);
    context.fill();
    context.stroke();
    
    // Draw dashed line to yAxis
    context.beginPath();
    var direction = (Math.cos(t) &lt; 0) ? 1 : -1;
    var start = (direction==-1) ? -5 : 0;
    for (var i = x;  direction*i &lt; direction*yAxis-5; i = i+direction*10) {
        context.moveTo(i+direction*5, y);
        context.lineTo(i+direction*10, y);
    }
    context.stroke();
    
    // Draw yAxis bead
    context.beginPath();
    context.arc(yAxis, y, 5, 0, 2*Math.PI, false);
    context.fill();
    context.stroke();
}

    Humble.Trig.init()
    
})();

&lt;/script&gt;

&lt;p&gt;Typically we are not interested in small lattice sizes and want to know what the behavior looks like in macroscopic systems. Defining an intrinsic quantity &lt;script type=&quot;math/tex&quot;&gt;\,f = \frac{1}{N\beta}\ln Z&lt;/script&gt; called the &lt;i&gt;free energy per spin&lt;/i&gt;, we can take a well-defined limit as the number of sites goes to infinity. For our 1D solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{N \rightarrow \infty}f = \ln2 + \ln \cosh K&lt;/script&gt;

&lt;p&gt;Which is completely smooth. No phase transition in 1D. It seems we’ll have to try something more complicated if we are to capture our elusive prey.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;https://www.youtube.com/watch?v=AT4_S9vQJgc&quot;&gt;Leonard Susskind give’s a very intuite explanation on why a phase transition cannot occur in 1D, making analogy with the game of telephone&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;criticality-in-2d&quot;&gt;Criticality in 2D&lt;/h2&gt;

&lt;p&gt;The full solution in two dimensions is very complicated to derive (apparently it took Richard Feynman 14 pages to explain it in his lectures on statistical mechanics). I won’t endeavor to try your patience to such an extent, but fortunately we can find the critical temperature in a slightly less painful manner, using a result called &lt;a href=&quot;http://en.wikipedia.org/wiki/Kramers%E2%80%93Wannier_duality&quot;&gt;&lt;i&gt;Kramers-Wannier Duality&lt;/i&gt;&lt;/a&gt;. Nevertheless, a certain amount of pain must be endured, so if you’re more algorithmically minded feel free to make a cowardly break for the next section.&lt;/p&gt;

&lt;p&gt;To begin we rewrite the partition function as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z = \cosh(K)^N Z^\prime &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z^\prime = \sum_{\{\sigma\}} \prod_{\langle i,j \rangle} [1 + \sigma_i \sigma_j \tanh(K)]&lt;/script&gt;

&lt;p&gt;using the fact that &lt;script type=&quot;math/tex&quot;&gt;e^{\pm x} = \cosh(x)[1 \pm \tanh(x)] &lt;/script&gt;. We can keep track of the terms combinitorically by matching the &lt;script type=&quot;math/tex&quot;&gt;\sigma_i \sigma_j \tanh(K)&lt;/script&gt; terms up to edges in a subgraph &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; of the lattice:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z^\prime = \sum_{\{P\}} \prod_{\langle i,j \rangle \in P} \sigma_i \sigma_j tanh(K)&lt;/script&gt;

&lt;p&gt;Consider a subgraph consisting of a single non-self-intersecting path and let &lt;script type=&quot;math/tex&quot;&gt;P(k)&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;th site along the path: &lt;/p&gt;

&lt;center&gt;
	&lt;img src=&quot;/images/kramers-wannier-duality-002.png&quot; height=&quot;200&quot; width=&quot;200&quot; /&gt;
&lt;/center&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \sum_{\{\sigma\}} \sigma_{i} \sigma_{P(1)}^2 \sigma_{P(2)}^2 \ldots \sigma_{P(L-1)}^2 \sigma_{f} = 0&lt;/script&gt;

&lt;p&gt;&lt;i&gt;unless&lt;/i&gt; &lt;script type=&quot;math/tex&quot;&gt;P(0) = P(L)&lt;/script&gt;, i.e.: &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; is a closed path. More generally, we can use this same idea to prove that the only link configurations that contribute to the partition function are those that form &lt;i&gt;even subgraphs&lt;/i&gt;. Using this fact we can further simplify &lt;script type=&quot;math/tex&quot;&gt;Z^\prime&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z^\prime = \sum_{l=0}^{2S} c(l) \tanh(K)^l &lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;c(l)&lt;/script&gt; counts the number of even subgraphs with length &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt;. This is one half of the duality, usually referred to as the &lt;i&gt;high temperature expansion&lt;/i&gt;.&lt;/p&gt;

&lt;p&gt;Now, consider the &lt;i&gt;dual&lt;/i&gt; lattice whose sites sit in the squares of the original lattice and whose neighbors are those squares it shares an edge with. When we use periodic boundary conditions the lattice forms a torus, and in fact the dual lattice forms an identical torus!&lt;/p&gt;

&lt;p&gt;Given an even subgraph on the original lattice there exists a two-to-one correspondence with spin configurations on the dual lattice:&lt;/p&gt;

&lt;center&gt;
	&lt;img src=&quot;/images/kramers-wannier-duality-001.png&quot; height=&quot;200&quot; width=&quot;500&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;At least, that is &lt;i&gt;almost&lt;/i&gt; true. In fact the correspondence breaks down for certain subgraphs which wrap completely around the torus, but these terms contribute negligbly to the summation and will go to zero in the thermodynamic limit.&lt;/p&gt;

&lt;p&gt;At low energy all of the spins in the dual lattice will be lined up, so the ground state energy is &lt;script type=&quot;math/tex&quot;&gt;H_0 = -KN&lt;/script&gt;. Thus, we can rewrite the original Hamiltonian as: &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H = H_0 + \sum_{\sigma_i \neq \sigma_j \vert \langle i,j \rangle} 2K&lt;/script&gt;

&lt;p&gt;Here the summation is only over the links connecting sites of differing spin, which occur precisely across each of the edges present in the subgraphs of the high temperature expansion; each edge lifing the energy by &lt;script type=&quot;math/tex&quot;&gt;2K&lt;/script&gt;. Note that the energy of a configuration again depends &lt;i&gt;only&lt;/i&gt; on the number of edges in the subgraph. This allows us to write the partition function in a suggestively familiar form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \hat{Z} = 2e^{\hat{K}N}\sum_{l=0}^{2N} c(l)e^{-2\hat{K}l} &lt;/script&gt;

&lt;p&gt;where the factor of 2 out front comes from the two-to-one correspondence between the dual lattice and even subgraphs on the original lattice. Behold: the &lt;i&gt;low temperature expansion&lt;/i&gt;! &lt;/p&gt;

&lt;p&gt;As we take &lt;script type=&quot;math/tex&quot;&gt;N \rightarrow \infty&lt;/script&gt; both expansions must be identical under the identification &lt;script type=&quot;math/tex&quot;&gt;\tanh(K) \leftrightarrow e^{-2\hat{K}}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \frac{\hat{Z}}{2e^{\hat{K}N}} = \frac{Z}{\cosh(K)^N} = \sum_{l} c(l)e^{-2\hat{K}l} = \sum_{l} c(l) \tanh(K)^l&lt;/script&gt;

&lt;p&gt;This identification defines a mapping between high temperature and low temperature which preserves analytic behavior. Since critical points are &lt;i&gt;defined&lt;/i&gt; by nonanalyticity, critical points must be mapped to critical points. &lt;/p&gt;

&lt;p&gt;If we assume the system contains only one phase transition, &lt;i&gt;it must be mapped to itself&lt;/i&gt;. Thus &lt;script type=&quot;math/tex&quot;&gt;K_c&lt;/script&gt; occurs when &lt;script type=&quot;math/tex&quot;&gt;K=\hat{K}&lt;/script&gt;, and manipulating the tanh term we get a polynomial in &lt;script type=&quot;math/tex&quot;&gt;\chi = e^{2K_c}&lt;/script&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\chi^2 - 2\chi - 1 = 0&lt;/script&gt;

&lt;p&gt;Solving this we find the critical temperature&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; T_c = \frac{2J}{k \ln(1+\sqrt2)} &lt;/script&gt;

&lt;p&gt;and can now go to sleep. . .But ho! I am not through with you yet:&lt;/p&gt;

&lt;h2 id=&quot;the-metropolis-algorithm&quot;&gt;The Metropolis Algorithm&lt;/h2&gt;

&lt;p&gt;Every physics treatment of this algorithm that I’ve read feels as if it’s described backwards, so I’m going to try doing things a bit differently.&lt;/p&gt;

&lt;p&gt;The overall idea is to build a markov chain whose equilibrium distribution of states matches the partition function. Let &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; denote the &lt;script type=&quot;math/tex&quot;&gt;2^Nx2^N&lt;/script&gt; transition matrix whose entries &lt;script type=&quot;math/tex&quot;&gt;P_{AB}&lt;/script&gt; represent the probability of transitioning from state &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; to state &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;i&gt;sufficient&lt;/i&gt;, but not &lt;i&gt;necessary&lt;/i&gt;, condition for the Markov chain to be in equilibrium is for it to satisfy the &lt;a href=&quot;http://en.wikipedia.org/wiki/Detailed_balance&quot;&gt;&lt;i&gt;detailed balance&lt;/i&gt;&lt;/a&gt; condition: &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \pi_A P_{AB} = \pi_B P_{BA}&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is the probability distribution over states. Since we want &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; to give the partition function, we must have &lt;script type=&quot;math/tex&quot;&gt;\pi_A = \frac{1}{Z}e^{-\beta E_A}&lt;/script&gt;. Thus: &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \frac{\pi_A}{\pi_B} = \frac{P_{BA}}{P_{AB}} = e^{\beta[E_B-E_A]} &lt;/script&gt;

&lt;p&gt;We have some freedom in choosing transition probabilities, they just need to satisfy this equation. So let’s define:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{AB} = \frac{1}{N} \mathcal{A}_{AB}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\mathcal{A}_{AB} = \left\{
     \begin{array}{lr}
       e^{\beta[E_B-E_A]} &amp; : B &lt; A\\
       1 &amp; : otherwise
     \end{array}
   \right.
 %]]&gt;&lt;/script&gt;

&lt;p&gt;for &lt;script type=&quot;math/tex&quot;&gt;A \neq B&lt;/script&gt; being nearest neighbors and &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; otherwise. Finally, the requirement that &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; be a stochastic matrix fixes &lt;script type=&quot;math/tex&quot;&gt;P_{AA} = 1 - \sum_{B \neq A}P_{AB} &lt;/script&gt;. &lt;/p&gt;

&lt;p&gt;Now that we know all this, we can finally define the &lt;a href=&quot;http://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&quot;&gt;&lt;i&gt;Metropolis Algorithm&lt;/i&gt;&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;initialize lattice with some spin configuration &lt;script type=&quot;math/tex&quot;&gt;\{\sigma\}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;choose &lt;script type=&quot;math/tex&quot;&gt;\sigma_i&lt;/script&gt; uniformly at random (drawing from &lt;script type=&quot;math/tex&quot;&gt;p(\sigma_i) = \frac{1}{N}&lt;/script&gt;)&lt;/li&gt;
  &lt;li&gt;set &lt;script type=&quot;math/tex&quot;&gt;\sigma_i := - \sigma_i&lt;/script&gt; defining the transition &lt;script type=&quot;math/tex&quot;&gt;A \rightarrow B&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;accept new state with probability &lt;script type=&quot;math/tex&quot;&gt;\mathcal{A}_{AB}&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;if new state is rejected reset &lt;script type=&quot;math/tex&quot;&gt;\sigma_i&lt;/script&gt; to its previous state&lt;/li&gt;
  &lt;li&gt;proceed to step (1)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since the algorithm initializes the spins in an arbitrary configuration there is a certain thermalization time needed before the system actually reaches equilibrium. Once equilibrium &lt;i&gt;is&lt;/i&gt; reached we can treat subsequent states as draws from the partition function. We will generally want each draw to be independent from the previous draw. To achieve this we can simply experiment to find a fixed number of iterations between draws for which the correlations are sufficiently close to zero.&lt;/p&gt;

&lt;h2 id=&quot;a-curious-link-to-quantum-field-theory&quot;&gt;A Curious Link to Quantum Field Theory&lt;/h2&gt;

&lt;p&gt;The starting point for most quantum field theory is the path integral, such as this one describing a real scalar field:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z = \int \mathcal{D}\phi e^{\frac{i}{h}\int dx^4 [\frac{1}{2}\partial_\mu\phi\partial^\mu\phi - \frac{1}{2}m^2\phi^2 - V(\phi)]} &lt;/script&gt;

&lt;p&gt;Fancifully put, a real scalar field is just the physics in the continuum limit of an infinitely large mattress. &lt;/p&gt;

&lt;p&gt;The path integral is essentially a sum over all paths in spacetime. The notation of &lt;script type=&quot;math/tex&quot;&gt;\partial_\mu\phi\partial^\mu\phi&lt;/script&gt; with &lt;a href=&quot;http://en.wikipedia.org/wiki/Einstein_notation&quot;&gt;one index raised and one lowered&lt;/a&gt; encodes the metric of &lt;a href=&quot;http://en.wikipedia.org/wiki/Minkowski_space&quot;&gt;Minkowski space&lt;/a&gt;. Described using this geometry the physics of special relativity essentially all boils down to the fact that you can convert space and time into each other by rotating through an imaginary angle. &lt;/p&gt;

&lt;p&gt;Since the functions being described here are analytic we can &lt;a href=&quot;http://en.wikipedia.org/wiki/Wick_rotation&quot;&gt;&lt;i&gt;rotate into imaginary time&lt;/i&gt;&lt;/a&gt; to do our calculations in euclidean space and then use &lt;a href=&quot;http://en.wikipedia.org/wiki/Analytic_continuation&quot;&gt;analytic continuation&lt;/a&gt; to put our results back into ordinary spacetime.&lt;/p&gt;

&lt;p&gt;But when you rotate into Euclidean space the exponent gets shuffled around:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Z = \int \mathcal{D}\phi e^{-\frac{1}{\hbar}\int dx^4 [\frac{1}{2}\delta^{\mu\nu}\partial_\mu\phi\partial_\nu\phi + \frac{1}{2}m^2\phi^2 + V(\phi)]}
= \int \mathcal{D}\phi e^{-\frac{1}{\hbar} H[\phi]} &lt;/script&gt;

&lt;p&gt;If you identify &lt;script type=&quot;math/tex&quot;&gt;\beta \leftrightarrow 1/\hbar&lt;/script&gt; this is exactly analogous to the partition function in statistical mechanics. This is an example of a &lt;i&gt;duality&lt;/i&gt;: every problem in quantum field theory has a corresponding problem in statistical mechanics, and vice versa. This greatly expands the variety of techniques at our disposal and has proven to be a powerful tool in physics. In particular the MCMC simulation we used here can be applied to quantum field theories and has been critical to understanding the &lt;a href=&quot;http://en.wikipedia.org/wiki/Lattice_QCD&quot;&gt;nonperturbative aspects of quantum chromodynamics&lt;/a&gt;. But that’s for a future post…&lt;/p&gt;

&lt;h2 id=&quot;links-and-resources&quot;&gt;Links and Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/steveKapturowski/QFT_Project&quot;&gt;[1] Source Code&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://micro.stanford.edu/~caiwei/me334/Chap12_Ising_Model_v04.pdf&quot;&gt;[2] Stanford Handout on the Ising Model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://math.arizona.edu/~tgk/541/chap3.pdf&quot;&gt;[3] The Renormalization Group for Ising Spins&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www-f1.ijs.si/~vilfan/SM/ln4b.pdf&quot;&gt;[4] More Renormalization Group Methods&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.peliti.org/Notes/ising2/vdov.html&quot;&gt;[5] Exact Solution of the 2D Ising Model&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://arxiv.org/pdf/hep-lat/0506036v1.pdf&quot;&gt;[6] Lattice QCD for Novices&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 16 May 2015 00:00:00 -0700</pubDate>
        <link>http://blog.stevenkapturowski.com/ising-model</link>
        <guid isPermaLink="true">http://blog.stevenkapturowski.com/ising-model</guid>
      </item>
    
      <item>
        <title>Understanding Similarity With Markov Chains</title>
        <description>&lt;p&gt;Facebook is a graph. Twitter is a graph. The internet is a graph. Almost any other kind of data you can think of probably has some sort of graph structure if you squint kinda funny and look at it from the right angle. So if you’re a data scientist, it’s pretty important to know how to deal with graphs. It’s a common question to ask how one can find things that are similar in a graph, but finding a good answer may not be as simple as you think.&lt;/p&gt;

&lt;h2 id=&quot;shortest-path&quot;&gt;Shortest Path&lt;/h2&gt;

&lt;p&gt;Starting off with a simple first guess we could choose to measure similarity by the shortest path connecting two nodes. In some ways this makes a lot of sense, particularly if you start thinking about your data as lying on some kind of submanifold in feature space (which you tend to do if you have an affinity for differential geometry). As it turns out, as the number of datapoints sampled from this hypothetical distribution goes to infinity you can meaningfully talk about the shortest path distance converging to the distance along a geodesic in the data manifold.&lt;/p&gt;

&lt;p&gt;Cool. But for certain types of graphs and applications this might not be good enough, and many naturally occurring graphs such as social networks and link graphs have degree distributions which roughly follow a power law: &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P(deg(v)=k) \sim k^{-\gamma} &lt;/script&gt;

&lt;p&gt;for some constant $\gamma$. Most vertices will have small degree, but the presence of vertices which connect to a substantial proportion of &lt;script type=&quot;math/tex&quot;&gt;\{v\}&lt;/script&gt; enables even the most disparate vertices to be connected by a short sequence of hops. This phenomena is well exemplified by the &lt;a href=&quot;http://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon&quot;&gt;“Six degrees of Kevin Bacon”&lt;/a&gt; game.&lt;/p&gt;

&lt;!-- ## Bipartite Graphs

Bipartite graphs exhibit a special structure whereby the vertices can be partitioned into two sets such that the only edges in G are those &lt;i&gt;between&lt;/i&gt; the two sets.

We can compute the singular value decomposition on large matrices via the Lanczos Method or Stochastic SVD --&gt;

&lt;h2 id=&quot;markov-chains&quot;&gt;Markov Chains&lt;/h2&gt;

&lt;p&gt;The picture to have in your head when you think about Markov Chains is just a (possibly directed) graph where every node’s outgoing edge weights sum to one and so can be interpreted as probabilities. &lt;/p&gt;

&lt;center&gt;
	&lt;img src=&quot;/images/sum_to_one.png&quot; height=&quot;220&quot; width=&quot;200&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Given any weighted graph we can always normalize the edge weights to satisfy this condition:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P = D^{-1}W &lt;/script&gt;

&lt;p&gt;where W is the weight matrix of the original graph, and D is a diagonal matrix defined by &lt;script type=&quot;math/tex&quot;&gt; D_{ii} = \sum_j W_{ij} &lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Starting at any vertex we can generate a random walk by repeatedly choosing a new vertex to move to according to the transition probabilities &lt;script type=&quot;math/tex&quot;&gt; P_{ij} &lt;/script&gt; for the edge leaving i and arriving at j. A Markov Chain is said to be &lt;i&gt;ergodic&lt;/i&gt; if (I) given any two states i and f there exists a path from i to f that can be traversed with nonzero probability, and (II) every state is &lt;i&gt;aperiodic&lt;/i&gt;. Note, the Markov chain above is not ergodic because it fails condition (I): you get trapped in either states j or k and can’t make your way back to the other states. I won’t rigorously define what periodicity is meant here because &lt;a href=&quot;http://en.wikipedia.org/wiki/Markov_chain&quot;&gt;wikipedia&lt;/a&gt; does a good enough job and this article is long enough as it is, but below are a couple of simple examples in which every state is periodic, with the periodicity of each state labeled:&lt;/p&gt;

&lt;center&gt;
	&lt;img src=&quot;/images/periodicity.png&quot; height=&quot;220&quot; width=&quot;200&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;Consider a vector &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; which represents a probability distribution over each of the vertices. If we start with all of the probability mass centered on one vertex  and iterate the distribution according to &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \pi_k = \pi_{k-1}P \tag{1}&lt;/script&gt;

&lt;p&gt;then in fact this is the random walk we just described. More generally we can start with any initial distribution and we’ll end up with a kind of weighted average over random walks with different starting points. &lt;/p&gt;

&lt;p&gt;A distribution &lt;script type=&quot;math/tex&quot;&gt;\pi_0&lt;/script&gt; is called an equilibrium distribution if it satisfies:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \pi_0^T = \pi_0^T P &lt;/script&gt;

&lt;h2 id=&quot;what-do-random-walks-give-us-that-shortest-paths-dont&quot;&gt;What do Random Walks Give us that Shortest Paths Don’t?&lt;/h2&gt;

&lt;p&gt;Consider a vertex i adjacent to a high degree vertex j, and a vertex k which has i as its only neighbor. Intuitively, it should make sense that &lt;script type=&quot;math/tex&quot;&gt; Sim(j, i) &gt; Sim(i, k) &lt;/script&gt; because their association is more uniquely distinguishing, but under path similarity they are the same.&lt;/p&gt;

&lt;p&gt;Consider a random walk starting at i. If it hops to vertex j then at the next step the probability distribution would get smeared out over &lt;i&gt;every&lt;/i&gt; vertex that j is connected to, so ultimately j will tend to end up with a low probability. Conversely, if the random walk hops over to k, then at the next step the &lt;i&gt;only&lt;/i&gt; choice is for it to come back to i. In other words, the random walk can tell that there’s less randomness associated with these connections!&lt;/p&gt;

&lt;p&gt;It’s possible to define a variety of different notions of similarity based on Markov chains, but they basically all capture this key feature. There’s no clear way to say which is theoretically &lt;i&gt;best&lt;/i&gt;, and if you have time it might be advisable to try several of them out and compare their performance on your particular data. &lt;/p&gt;

&lt;p&gt;Below I discuss some possible choices, how to compute them, and some of their important properties.&lt;/p&gt;

&lt;h2 id=&quot;pagerank&quot;&gt;PageRank&lt;/h2&gt;

&lt;p&gt;We can compute the personalized PageRank for some initial distribution via a power iteration as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \pi_t^T = \beta \pi_0^T + (1-\beta) \pi_{t-1}^T P &lt;/script&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# pi0 is any initial distribution&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;personalizedPageRank&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;D_inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dia_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;D_inv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setdiag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_inv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;pi_n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;pi_n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi_n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi_n&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you don’t need or want a symmetric notion of similarity then this might be enough: given &lt;script type=&quot;math/tex&quot;&gt;\pi_0&lt;/script&gt; centered on vertex &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; you compute the PageRank and you take the vertices assigned the highest probability as the most similar.&lt;/p&gt;

&lt;p&gt;Alternatively, given any &lt;i&gt;two&lt;/i&gt; vertices, we compute their PageRank’s separately, so each vertex gets transformed into a probability distribution. Now we simply use some standard method for comparing probability distributions such as &lt;a href=&quot;http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&quot;&gt;KL Divergence&lt;/a&gt; (which is also non-symmetric) or &lt;a href=&quot;http://en.wikipedia.org/wiki/Bhattacharyya_distance&quot;&gt;Bhattacharyya distance&lt;/a&gt; (which &lt;i&gt;is&lt;/i&gt; symmetric).&lt;/p&gt;

&lt;h2 id=&quot;commute-times-and-hitting-times&quot;&gt;Commute Times and Hitting Times&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Hitting_time&quot;&gt;Hitting times&lt;/a&gt; are a very interesting property that can capture a considerable amount of the connectivity structure of a graph. But depending on our application they might not map to the notion of similariy we want because it’s not symmetric between vertices, i.e.: the hitting time from a to b is not the same as the hitting time from b to a. Fortunately there a very easy way to fix this: define the &lt;i&gt;commute time&lt;/i&gt; &lt;script type=&quot;math/tex&quot;&gt; C_{ij} \equiv h_{ij} + h_{ji} &lt;/script&gt; as the expected time to travel from i to j and then return back to i. &lt;/p&gt;

&lt;p&gt;Great! Now that we have a nice, symmetric notion of Markov similarity, how do we compute it?&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; h_{ij} = \frac{L_{jj}^\dagger}{\pi_j} - \frac{L_{ij}^\dagger}{\sqrt{\pi_i \pi_j}} &lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt; L^\dagger = (L^T L)^{-1}L^T &lt;/script&gt; is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse&quot;&gt;&lt;i&gt;Moore-Penrose pseudoinverse&lt;/i&gt;&lt;/a&gt;, &lt;script type=&quot;math/tex&quot;&gt;L = I - D^{-1/2} W D^{-1/2}&lt;/script&gt; is the &lt;a href=&quot;http://en.wikipedia.org/wiki/Laplacian_matrix&quot;&gt;&lt;i&gt;normalized graph Laplacian&lt;/i&gt;&lt;/a&gt;, and &lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt; is the stationary distribution of &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt;. Now personally, I think this is a very cool formula. The only problem is that it’s totally useless for most practical calculations. Assuming the graph we’re dealing with is large enough to require exploiting its sparsity we’re out of luck because this formula relies entirely on computing the inverse of a very large, sparse matrix. And generically the inverse of a sparse matrix will be dense.&lt;/p&gt;

&lt;p&gt;This would seem to put us in a difficult position, but fear not brave programmer, for all hope is not lost!&lt;/p&gt;

&lt;h2 id=&quot;power-iteration&quot;&gt;Power Iteration&lt;/h2&gt;

&lt;p&gt;The hitting time can be computed by the following expectation value:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; h_{ij} = \sum_{t=0}^\infty t(P^{t})_{ij}[1 - \sum_{k=0}^{t-1}(P^k)_{ij}] &lt;/script&gt;

&lt;p&gt;Truncating this series at some &lt;script type=&quot;math/tex&quot;&gt;T-1&lt;/script&gt; and adding in a remainder term we get the &lt;i&gt;truncated hitting time&lt;/i&gt;:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; h_{ij}(T) = \bigg(\sum_{t=0}^{T-1} t(P^{t})_{ij}[1 - \sum_{k=0}^{t-1}(P^k)_{ij}]\bigg) + T[1-\sum_{k=0}^{T-1}(P^k)_{ij}] &lt;/script&gt;

&lt;p&gt;Finally, we can rewrite the above equation as follows to avoid having to compute powers of the transition matrix:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \rho_k^T(\hat{e}_i) = \rho_{k-1}^T(\hat{e}_i) P \mid \rho_0 = \hat{e}_i &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; h_{ij}(T) = \bigg(\sum_{t=0}^{T-1}t(\rho_t^T)_j[1 - \sum_{k=0}^{t-1}(\rho_k^T)_j]\bigg) + T[1-\sum_{k=0}^{T-1}(\rho_k^T)_j] &lt;/script&gt;

&lt;p&gt;This approximation will always underestimate the true hitting time, as all the unused probabily mass after &lt;script type=&quot;math/tex&quot;&gt;T-1&lt;/script&gt; gets lumped into the &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; term. Thus we’ll have to play with cutoff to make sure the range &lt;script type=&quot;math/tex&quot;&gt;[0, T]&lt;/script&gt; has sufficient resolution to capture most of the interesting behavior.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;truncatedHittingTimes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;D_inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dia_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;D_inv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setdiag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([(&lt;/span&gt;&lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D_inv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;one_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;rho_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;used_probability&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

	&lt;span class=&quot;n&quot;&gt;rho_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;used_probability&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;rho_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;currentProbability&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rho_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;used_probability&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;used_probability&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currentProbability&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currentProbability&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;used_probability&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;coda&quot;&gt;Coda&lt;/h2&gt;

&lt;p&gt;This still isn’t the end of the story, as e.g. &lt;a href=&quot;https://arxiv.org/pdf/1003.1266.pdf&quot;&gt;Luxburg et al. 2011&lt;/a&gt; study the behavior of commute times in certain types of random graph and prove that as the size of these graphs tends to infinity the commute times converge to purely local quantities with respect to the considered vertices. Thus for such graphs commute times may fail to convey any meaninful information as a distance measure! &lt;a href=&quot;http://www.math.pku.edu.cn/teachers/yaoy/Fall2011/lecture10.pdf&quot;&gt;Diffusion distance&lt;/a&gt; could be a good alternative if you’re faced with this situation, but there’s an important moral here: much like machine learning algorithms there’s no free lunch when it comes to measuring distance on graphs. Always think about the failure cases and never try just one.&lt;/p&gt;

&lt;h2 id=&quot;links-and-resources&quot;&gt;Links and Resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.cis.upenn.edu/~ccb/publications/graph-laplacian-affinity-measures.pdf&quot;&gt;D. Rao, D. Yarowsky, C. Callison-Burch, Affinity Measures based on the Graph Laplacian.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jmlr.org/papers/volume15/vonluxburg14a/vonluxburg14a.pdf&quot;&gt;U. Luxburg, A. Radl, M. Hein, Hitting and Commute Times in Large Random Neighborhood Graphs.&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 01 May 2015 00:00:00 -0700</pubDate>
        <link>http://blog.stevenkapturowski.com/understanding-similarity</link>
        <guid isPermaLink="true">http://blog.stevenkapturowski.com/understanding-similarity</guid>
      </item>
    
  </channel>
</rss>
